{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wav2vec(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Wav2vec, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 3x3 square convolution\n",
    "        # kernel\n",
    "        activation = nn.ReLU()\n",
    "        dropout = 0.0\n",
    "        self.encoder = Encoder(activation, dropout)\n",
    "        self.context = Context(10,10,3,0.5,nn.ReLU())\n",
    "        \n",
    "        # Calculate offset for prediction module\n",
    "        def calc_offset(): \n",
    "            jin = 0\n",
    "            rin = 0\n",
    "            for layer in next(self.encoder.children()):\n",
    "                if layer.__class__.__name__ == 'Conv1d': \n",
    "                    k = layer.kernel_size[0]\n",
    "                    stride = layer.stride[0]\n",
    "                    if rin == 0:\n",
    "                        rin = k\n",
    "                    rin = rin + (k - 1) * jin\n",
    "                    if jin == 0:\n",
    "                        jin = stride\n",
    "                    else:\n",
    "                        jin *= stride\n",
    "            offset = math.ceil(rin / jin)\n",
    "\n",
    "            return int(offset)\n",
    "        \n",
    "        self.offset = calc_offset()\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        c = self.context(z)\n",
    "        # x = x.view(-1, self.num_flat_features(x))\n",
    "        return z, c\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, activation, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.in_c = 10\n",
    "        \n",
    "        # Hardcoded architecture, as the blocks are different\n",
    "        self.encoder = nn.Sequential(nn.Conv1d(in_channels=1, out_channels=self.in_c, kernel_size=10, stride=5),\n",
    "                                     nn.Dropout(p=dropout),\n",
    "                                     nn.GroupNorm(1, self.in_c),  # Affine, what to do?\n",
    "                                     activation,\n",
    "                                     # 2nd layer\n",
    "                                     nn.Conv1d(in_channels=self.in_c, out_channels=self.in_c, kernel_size=8, stride=4),\n",
    "                                     nn.Dropout(p=dropout),\n",
    "                                     ## See norm_block - FB_repo\n",
    "                                     nn.GroupNorm(1, self.in_c),  # Affine, what to do?\n",
    "                                     activation,\n",
    "                                     # 3rd layer\n",
    "                                     nn.Conv1d(in_channels=self.in_c, out_channels=self.in_c, kernel_size=4, stride=2),\n",
    "                                     nn.Dropout(p=dropout),\n",
    "                                     nn.GroupNorm(1, self.in_c),  # Affine, what to do?\n",
    "                                     activation,\n",
    "                                     # Fourth layer\n",
    "                                     nn.Conv1d(in_channels=self.in_c, out_channels=self.in_c, kernel_size=4, stride=2),\n",
    "                                     nn.Dropout(p=dropout),\n",
    "                                     nn.GroupNorm(1, self.in_c),  # Affine, what to do?\n",
    "                                     activation,\n",
    "                                     # Fifth layer\n",
    "                                     nn.Conv1d(in_channels=self.in_c, out_channels=self.in_c, kernel_size=4, stride=2),\n",
    "                                     nn.Dropout(p=dropout),\n",
    "                                     nn.GroupNorm(1, self.in_c),  # Affine, what to do?\n",
    "                                     activation)\n",
    "    def log_compression(self, x):\n",
    "        # https://www.edn.com/log-1-x-compression/\n",
    "        x = x.abs()\n",
    "        x = x + 1\n",
    "        return x.log()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.log_compression(x)\n",
    "        # TODO implement skipped connections?\n",
    "        return x\n",
    "    \n",
    "class Context(nn.Module):\n",
    "    def __init__(self, n_in, n_out, k,dropout, activation, layers=10):\n",
    "        super(Context, self).__init__()\n",
    "\n",
    "        # All block are the same, so create using a function\n",
    "        def conv_block(n_in, n_out, k, dropout, activation):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv1d(n_in, n_out, k, padding=1),\n",
    "                nn.Dropout(p=dropout),\n",
    "                nn.GroupNorm(1, n_out),\n",
    "                activation\n",
    "            )\n",
    "\n",
    "        # Holder for conv layers\n",
    "        self.conv = nn.ModuleList()\n",
    "        \n",
    "        # Create #layers number of conv-blocks\n",
    "        for i in range(0, layers):\n",
    "            self.conv.append(conv_block(n_in, n_out, k, dropout, activation))\n",
    "        self.conv = nn.Sequential(*self.conv)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Wav2vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform, sample_rate = torchaudio.load(\"wav_16k_example.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 31440])"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unsqueeze(waveform, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162.06185567010309"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(31440/194)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.022"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(162*31/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Wav2vec()\n",
    "z,c = m(waveform.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 194])"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# z \n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 194])"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prediction(nn.Module):\n",
    "    def __init__(self, predictions_steps):\n",
    "        in_dim = 10 # antal channels fra aggregator UPDATE TO USE AS ARGUMENT IN CONSTRUCTOR\n",
    "        out_dim = 10 # antal outchannels fra encoder UPDATE TO USE AS ARGUMENT IN CONSTRUCTOR\n",
    "        prediction_steps = 12 # Not sure what this is? It's an argument UPDATE TO USE AS ARGUMENT IN CONSTRUCTOR\n",
    "        super(Prediction, self).__init__()\n",
    "        self.transpose_context = nn.ConvTranspose2d(in_dim, out_dim, (1, prediction_steps))\n",
    "        self.sample_distance = None\n",
    "        self.n_negatives = 1\n",
    "        \n",
    "        \n",
    "    def sample_negatives(self, y): \n",
    "        bsz, fsz, tsz = y.shape\n",
    "\n",
    "        y = y.transpose(0, 1)  # BCT -> CBT\n",
    "        y = y.contiguous().view(fsz, -1)  # CBT => C(BxT)\n",
    "\n",
    "        cross_high = tsz * bsz\n",
    "        high = tsz if self.sample_distance is None else min(tsz, self.sample_distance)\n",
    "        assert high > 1\n",
    "\n",
    "        neg_idxs = torch.randint(low=0, high=high, size=(bsz, self.n_negatives * tsz))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if self.n_negatives > 0:\n",
    "                tszs = (\n",
    "                    # REMEMBER TO INCLUDE BUFFERED_ARANGE FROM UTIL\n",
    "                    buffered_arange(tsz)\n",
    "                    .unsqueeze(-1)\n",
    "                    .expand(-1, self.n_negatives)\n",
    "                    .flatten()\n",
    "                )\n",
    "\n",
    "                neg_idxs = torch.randint(\n",
    "                    low=0, high=high - 1, size=(bsz, self.n_negatives * tsz)\n",
    "                )\n",
    "                neg_idxs[neg_idxs >= tszs] += 1\n",
    "\n",
    "\n",
    "        for i in range(1, bsz):\n",
    "            neg_idxs[i] += i * high\n",
    "\n",
    "        negs = y[..., neg_idxs.view(-1)]\n",
    "        negs = negs.view(\n",
    "            fsz, bsz, self.n_negatives + 0, tsz\n",
    "        ).permute(\n",
    "            2, 1, 0, 3\n",
    "        )  # to NxBxCxT\n",
    "        return negs\n",
    "\n",
    "    \n",
    "    def forward(self, c, z):\n",
    "        c = c.unsqueeze(-1)\n",
    "        # Transpose to give steps predictions into the future\n",
    "        c = self.transpose_context(c)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buffered_arange(max):\n",
    "    if not hasattr(buffered_arange, \"buf\"):\n",
    "        buffered_arange.buf = torch.LongTensor()\n",
    "    if max > buffered_arange.buf.numel():\n",
    "        buffered_arange.buf.resize_(max)\n",
    "        torch.arange(max, out=buffered_arange.buf)\n",
    "    return buffered_arange.buf[:max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim = 10 # antal channels fra aggregator \n",
    "out_dim = 10 # antal outchannels fra encoder\n",
    "prediction_steps = 12 # Not sure what this is? It's an argument \n",
    "hk = nn.ConvTranspose2d(in_dim, out_dim, (1, prediction_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 194])"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 194, 12])"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hk(c.unsqueeze(-1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1214,  1.1560,  0.8778, -0.2357]])"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.normal(0.5, 0.5, size=(1, 4,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4])"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1,2])\n",
    "b = torch.tensor([3,4])\n",
    "\n",
    "torch.cat([a,b], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 194])"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 10, 194])"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = c.clone()\n",
    "y = z.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_distance = None\n",
    "n_negatives = 1\n",
    "def sample_negatives(y): \n",
    "    bsz, fsz, tsz = y.shape\n",
    "\n",
    "    y = y.transpose(0, 1)  # BCT -> CBT\n",
    "    y = y.contiguous().view(fsz, -1)  # CBT => C(BxT)\n",
    "\n",
    "    cross_high = tsz * bsz\n",
    "    high = tsz if sample_distance is None else min(tsz, sample_distance)\n",
    "    assert high > 1\n",
    "\n",
    "    neg_idxs = torch.randint(low=0, high=high, size=(bsz, n_negatives * tsz))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if n_negatives > 0:\n",
    "            tszs = (\n",
    "                # REMEMBER TO INCLUDE BUFFERED_ARANGE FROM UTIL\n",
    "                buffered_arange(tsz)\n",
    "                .unsqueeze(-1)\n",
    "                .expand(-1, n_negatives)\n",
    "                .flatten()\n",
    "            )\n",
    "\n",
    "            neg_idxs = torch.randint(\n",
    "                low=0, high=high - 1, size=(bsz, n_negatives * tsz)\n",
    "            )\n",
    "            neg_idxs[neg_idxs >= tszs] += 1\n",
    "\n",
    "\n",
    "    for i in range(1, bsz):\n",
    "        neg_idxs[i] += i * high\n",
    "\n",
    "    negs = y[..., neg_idxs.view(-1)]\n",
    "    negs = negs.view(\n",
    "        fsz, bsz, n_negatives + 0, tsz\n",
    "    ).permute(\n",
    "        2, 1, 0, 3\n",
    "    )  # to NxBxCxT\n",
    "    return negs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 194])"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST FUNCTION FOR PROJECT_STEPS\n",
    "in_dim = 10 # antal channels fra aggregator \n",
    "out_dim = 10 # antal outchannels fra encoder\n",
    "prediction_steps = 12 # Not sure what this is? It's an argument \n",
    "hk = nn.ConvTranspose2d(in_dim, out_dim, (1, prediction_steps))\n",
    "x = x.unsqueeze(-1)\n",
    "test = hk(x)\n",
    "# Unsqueeze before transposing (creating prediction_steps)\n",
    "\n",
    "x = hk(x)  # self.project_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
